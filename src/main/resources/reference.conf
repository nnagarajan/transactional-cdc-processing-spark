# Shared defaults for all jobs
spark {
  dev {
    master = "local[*]"
    app-name = "dev-launch"
    enable-hive-support = true
  }

  extensions {
    sql-extensions = "io.delta.sql.DeltaSparkSessionExtension"
    catalog = "org.apache.spark.sql.delta.catalog.DeltaCatalog"
  }

  state-store.provider-class = "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider"

  resources {
    executor-memory = "1G"
    driver-memory = "1G"
    driver-cores = 4
  }

  ui-port = "4041"
  warehouse-dir = "/tmp/warehouse/sql2"

  hive {
    connection-url = "jdbc:derby:/tmp/warehouse/metastore_db1;create=true"
    connection-driver = "org.apache.derby.jdbc.EmbeddedDriver"
  }

  log-level = "WARN"
}

kafka {
  starting-offsets = "earliest"
  fail-on-data-loss = false
}
